{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# Q-Learning with Table ü§ñ üéõ üçΩ üöï\n",
        "> This is an exercise of Q-learning at its simplest for solving Reinforcement Learning (RL) problems. The challenge here is a \"Taxi-V3\" that is being solved via **table** approach for Q-learning implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ONOODsyrMU"
      },
      "source": [
        "## A small recap of Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68VveLacfxJ"
      },
      "source": [
        "- The *Q-Learning* **is the RL algorithm that**  \n",
        "\n",
        "  - Trains *Q-Function*, an **action-value function** that contains, as internal memory, a *Q-table* **that contains all the state-action pair values.**\n",
        "    \n",
        "  - Given a state and action, our Q-Function **will search into its Q-table the corresponding value.**\n",
        "\n",
        "  - When the training is done,**we have an optimal Q-Function, so an optimal Q-Table.**\n",
        "    \n",
        "  - And if we **have an optimal Q-function**, we\n",
        "have an optimal policy,since we **know for each state, what is the best action to take.**\n",
        "\n",
        "This is the Q-Learning pseudocode:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      },
      "source": [
        "# Let's code our Q-Learning with Table üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gpxC1_kqUYe"
      },
      "source": [
        "## Install dependencies and create a virtual display üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v4DVePLwiW5"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfbs\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6XC13pTfFiD"
      },
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks for this trick, **we will be able to run our virtual screen.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Import the packages üì¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle5 as pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lN8Bz7yvLt"
      },
      "source": [
        "# Environment: Taxi-v3 üöñ\n",
        "\n",
        "## Create and understand [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "---\n",
        "\n",
        "üí° A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "---\n",
        "\n",
        "In `Taxi-v3` üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\n",
        "\n",
        "When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJnb8O3y8up"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## Define the hyperparameters ‚öôÔ∏è\n",
        "The exploration related hyperparamters are some of the most important ones.\n",
        "\n",
        "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
        "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 25000   # Total training episodes\n",
        "learning_rate = 0.7           # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# DO NOT MODIFY EVAL_SEED\n",
        "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
        " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
        " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
        "                                                          # Each seed has a specific starting state\n",
        "\n",
        "# Environment parameters\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05           # Minimum exploration probability\n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vucwbse4OV0T"
      },
      "source": [
        "# Q1: Let's see what the Environment looks like (10 pts)\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR-2Uefq_yZf"
      },
      "outputs": [],
      "source": [
        "print(\"_____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space\", env.action_space)\n",
        "print(\"Sample observation\", env.action_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r50Advrh5Q"
      },
      "source": [
        "Reward function üí∞:\n",
        "\n",
        "- -1 per step unless other reward is triggered.\n",
        "- +20 delivering passenger.\n",
        "- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjIqVFYJOmJC"
      },
      "source": [
        "## Q1.1 Why the observation space shape is 500? (5 pts)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V9jJMIFPH7R"
      },
      "source": [
        "## Write Down Your Answer Here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQTfLw_UPzje"
      },
      "source": [
        "## Q1.2 What are the possible actions the agent can take? (5 pts)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqycQKGTP-j-"
      },
      "source": [
        "## Write Down Your Answer Here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "# Q2: Create and Initialize the Q-table (10 pts)\n",
        "(üëÄ Step 1 of the pseudocode)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3ZCdluj3k0l"
      },
      "outputs": [],
      "source": [
        "state_space = 500\n",
        "action_space = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhMsaprxTxWF"
      },
      "source": [
        "\n",
        "### Q2.1: Fill in function `initialize_q_table` below (10 pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCddoOXM3UQH"
      },
      "outputs": [],
      "source": [
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable =\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YfvrqRt3jdR"
      },
      "outputs": [],
      "source": [
        "Qtable_taxi = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "# Q3: Define the epsilon-greedy policy (20 pts)\n",
        "\n",
        "(üëÄ Step 2 of the pseudocode)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sRirfbWUdov"
      },
      "source": [
        "### Q3.1: Fill in function `epsilon_greedy_policy` below (20 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action =\n",
        "\n",
        "  return action\n",
        "\n",
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num =\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action =\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action =  # Take a random action\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "# Q4: Create the training loop method (40 pts)\n",
        "\n",
        "The training loop goes like this:\n",
        "\n",
        "```\n",
        "For episode in the total of training episodes:\n",
        "\n",
        "Reduce epsilon (since we need less and less exploration)\n",
        "Reset the environment\n",
        "\n",
        "  For step in max timesteps:    \n",
        "    Choose the action At using epsilon greedy policy\n",
        "    Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    If done, finish the episode\n",
        "    Our next state is the new state\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfMISA6vY0DC"
      },
      "source": [
        "## Q4.1: Fill in function `train` below (25 pts)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  # store the training progress of this algorithm for each episode\n",
        "  episode_steps = []\n",
        "  episode_resolveds = []\n",
        "\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action =\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, done, info =\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] =\n",
        "\n",
        "      # If done, finish the episode\n",
        "      if done:\n",
        "\n",
        "        # -> store the collected rewards & number of steps in this episode\n",
        "        episode_steps.append(step)\n",
        "        episode_resolveds.append(1)\n",
        "\n",
        "        step = 0\n",
        "\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "\n",
        "    if step != 0:\n",
        "      # -> store the collected rewards & number of steps in this episode\n",
        "      episode_steps.append(step)\n",
        "      episode_resolveds.append(0)\n",
        "\n",
        "      step = 0\n",
        "\n",
        "  return Qtable, episode_steps, episode_resolveds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "### Train the Q-Learning agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "Qtable_taxi, episode_steps, episode_resolveds = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef1NFNtLLWqa"
      },
      "source": [
        "### Training Progress\n",
        "\n",
        "In the generated graph, you will see red dots and a gree curve.\n",
        "\n",
        "Red dots indicate how many steps to take to reach the goal at different eposides.\n",
        "\n",
        "Green curve indicate whether the task is solved or not at different eposides.\n",
        "\n",
        "If Q-learning is trained properly, you will observe it is more likely this task can be solved with the increase of training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oy2JbHQl8kx"
      },
      "outputs": [],
      "source": [
        "# plot the rewards and number of steps over all training episodes\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "# ax1.plot(np.clip(episode_rewards, -30, 20), '-g', label = 'reward')\n",
        "# ax1.set_yticks([-10,20])\n",
        "ax1.plot(episode_resolveds, '-g', label = 'resolved')\n",
        "ax1.set_yticks([0,1])\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(np.clip(episode_steps, 0, 30), '+r', label = 'step')\n",
        "ax1.set_xlabel(\"episode\")\n",
        "ax1.set_ylabel(\"resolved\")\n",
        "ax2.set_ylabel(\"step\")\n",
        "ax1.legend(loc=2)\n",
        "ax2.legend(loc=1)\n",
        "plt.title(\"Training Progress\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## Q4.2: What can you learn from the training progress above? (15 pts)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82GU5lnJL5LH"
      },
      "source": [
        "### Write Down Your Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "# Q5: Evaluation (20 pts)\n",
        "\n",
        "- We defined the evaluation method that we're going to use to test our Q-Learning agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  episode_penalties = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    total_penalties_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if reward == -10:\n",
        "          total_penalties_ep += 1\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "    episode_penalties.append(total_penalties_ep)\n",
        "\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "  mean_penalty = np.mean(episode_penalties)\n",
        "  std_penalty = np.std(episode_penalties)\n",
        "\n",
        "  return mean_reward, std_reward, mean_penalty, std_penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## Evaluate our Q-Learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [],
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward, mean_penalty, std_penalty = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_taxi, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "print(f\"Mean_penalty={mean_penalty:.2f} +/- {std_penalty:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUI0N09AVJmZ"
      },
      "source": [
        "## Q5.1: What can you learn from the evaluation results above? (15 pts)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5wHkA3YWzsR"
      },
      "source": [
        "### Write Down Your Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs_0rTWRWYUb"
      },
      "source": [
        "## Q5.2: Record Agent (5 pts)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5LrR-joIHD"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset(seed=100)\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zhxvspJbke0"
      },
      "outputs": [],
      "source": [
        "record_video(env, Qtable_taxi, './replay.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cvndrlXKbo"
      },
      "source": [
        "**Your taxi should successfully pick up and drop off passenger in your recorded video.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4MHXszdZrSh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
