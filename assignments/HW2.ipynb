{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjRWziAVU2lZ"
   },
   "source": [
    "# REINFORCE with PyTorch \n",
    "\n",
    "This is an implementation exercise of applying the policy gradient method: REINFORCE with PyTorch. The challenge here is a \"PixelcopterEnv\" that is being solved with policy based methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsh4ZAamchSl"
   },
   "source": [
    "# Coding Reinforce algorithm from scratch\n",
    "\n",
    "The goal of your agent is to achieve a return of >= 5 for the PixelCopter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Installing dependencies for virtual display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr-Nuyb1dBm0"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjrLfPFIW8XK"
   },
   "source": [
    "## Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8ZVi-uydpgL"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAHAq6RZW3rn"
   },
   "source": [
    "## Import the packages \n",
    "In addition to import the installed libraries, we also import:\n",
    "\n",
    "- `imageio`: A library that will help us to generate a replay video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8oadoJSWp7C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfxJYdMeeVgv"
   },
   "source": [
    "## Check if we have a GPU\n",
    "\n",
    "- Let's check if we have a GPU\n",
    "- If it's the case you should see `device:cuda0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaJu5FeZxXGY"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5TNYa14aRav"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBPecCtBL_pZ"
   },
   "source": [
    "We're now ready to implement our Reinforce algorithm 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLVmKKVKA6j"
   },
   "source": [
    "## PixelCopter 🚁\n",
    "\n",
    "💡 A good habit when you start to use an environment is to check its documentation \n",
    "\n",
    "- [The Environment documentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Let's see what the Environment looks like (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBSc8mlfyin3"
   },
   "outputs": [],
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5u_zAHsKBy7"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7yJM9YXKNbq"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.1 What are the possible actions and observations ? (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 What is the terminal state for this environment ? (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV1466QP8crz"
   },
   "source": [
    "# Q2 Defining the Policy with Neural Networks (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1 Fill the missing portions of this code marked by \"Code Here\" ? (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1eBkCiX2X_S"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Define the three layers here\n",
    "        # Code Here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward process here (with ReLU activation for the first 2 layers)\n",
    "        # x -> fc1 -> ReLU -> fc2 -> ReLU -> fc3 \n",
    "        # Code Here\n",
    "\n",
    "        # We output the softmax\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce algorithm Pseudocode\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Defining the Reinforce algorithm and Training (55 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1 Fill the missing code (marked with \"Code Here\") for the Reinforce algorithm (40 pts)\n",
    "\n",
    "Note: There are 4 spots where you need to make the code edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOdv8Q9NfLK7"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = # Code Here: reset the environment\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = # Code Here: get the action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = # Code Here: take an env step\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        \n",
    "        ## We compute this starting from the last timestep to the first, to avoid redundant computations\n",
    "        \n",
    "        ## appendleft() function of queues appends to the position 0\n",
    "        ## We use deque instead of lists to reduce the time complexity\n",
    "        \n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(    ) # Code Here: complete here        \n",
    "       \n",
    "        ## standardization for training stability\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "        ## eps is added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM1QiGCSbBkM"
   },
   "source": [
    "### Defining the hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0uujOR_ypB6"
   },
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 10000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyvXTJWm9GJG"
   },
   "source": [
    "###  Train it\n",
    "- We're now ready to train our agent 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mM2P_ckysFE"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "torch.manual_seed(50) # Don't change this\n",
    "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HEqP-fy-Rf"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy,\n",
    "                   pixelcopter_optimizer,\n",
    "                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n",
    "                   pixelcopter_hyperparameters[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters[\"gamma\"], \n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2 What can you learn from the training progress above ? (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.3 Modify the hyperparamter h_size and comment on how the training is impacted by this change in h_size ? (5 pts)\n",
    "\n",
    "We have added the two sizes which we want you to experiment. You are free to try changing to other h_sizes and even change other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters_32 = {\n",
    "    \"h_size\": 32,\n",
    "    \"n_training_episodes\": 10000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mM2P_ckysFE"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "torch.manual_seed(50) # Don't change this\n",
    "pixelcopter_policy_32 = Policy(pixelcopter_hyperparameters_32[\"state_space\"], pixelcopter_hyperparameters_32[\"action_space\"], pixelcopter_hyperparameters_32[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer_32 = optim.Adam(pixelcopter_policy_32.parameters(), lr=pixelcopter_hyperparameters_32[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HEqP-fy-Rf"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy_32,\n",
    "                   pixelcopter_optimizer_32,\n",
    "                   pixelcopter_hyperparameters_32[\"n_training_episodes\"], \n",
    "                   pixelcopter_hyperparameters_32[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters_32[\"gamma\"], \n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters_128 = {\n",
    "    \"h_size\": 128,\n",
    "    \"n_training_episodes\": 10000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mM2P_ckysFE"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "torch.manual_seed(50) # Don't change this\n",
    "pixelcopter_policy_128 = Policy(pixelcopter_hyperparameters_128[\"state_space\"], pixelcopter_hyperparameters_128[\"action_space\"], pixelcopter_hyperparameters_128[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer_128 = optim.Adam(pixelcopter_policy_128.parameters(), lr=pixelcopter_hyperparameters_128[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HEqP-fy-Rf"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy_128,\n",
    "                   pixelcopter_optimizer_128,\n",
    "                   pixelcopter_hyperparameters_128[\"n_training_episodes\"], \n",
    "                   pixelcopter_hyperparameters_128[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters_128[\"gamma\"], \n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Evaluation (25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation method\n",
    "- Here we have defined the evaluation method that we're going to use to test the Reinforce agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FamHmxyhBEU"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param policy: The Reinforce agent\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      action, _ = policy.act(state)\n",
    "      new_state, reward, done, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohGSXDyHh0xx"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               pixelcopter_hyperparameters[\"max_t\"], \n",
    "               pixelcopter_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               pixelcopter_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.1: What can you learn from the evaluation results above? (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.2 Run the evaluation with the policies with different h_sizes (32 and 128) and comment on the results (10 pts)\n",
    "\n",
    "You are free to experiment with other h_sizes as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               pixelcopter_hyperparameters_32[\"max_t\"], \n",
    "               pixelcopter_hyperparameters_32[\"n_evaluation_episodes\"],\n",
    "               pixelcopter_policy_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               pixelcopter_hyperparameters_128[\"max_t\"], \n",
    "               pixelcopter_hyperparameters_128[\"n_evaluation_episodes\"],\n",
    "               pixelcopter_policy_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.3: Record Agent (5 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo4JH45if81z"
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []  \n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  img = env.render(mode='rgb_array')\n",
    "  images.append(img)\n",
    "  while not done:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_video(eval_env, pixelcopter_policy, './replay.mp4')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BPLwsPajb1f8",
    "L_WSo0VUV99t",
    "mjY-eq3eWh9O",
    "JoTC9o2SczNn",
    "gfGJNZBUP7Vn",
    "YB0Cxrw1StrP",
    "47iuAFqV8Ws-",
    "x62pP0PHdA-y"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
