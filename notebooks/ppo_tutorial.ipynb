{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proximal Policy Optimization (PPO) Tutorial\n",
        "\n",
        "## A Deep Dive into State-of-the-Art Policy Gradient Methods\n",
        "\n",
        "This tutorial provides a comprehensive guide to Proximal Policy Optimization (PPO), one of the most successful and widely-used policy gradient algorithms in modern reinforcement learning.\n",
        "\n",
        "### Table of Contents\n",
        "1. [Introduction to PPO](#introduction)\n",
        "2. [Mathematical Foundations](#mathematical)\n",
        "3. [Implementation](#implementation)\n",
        "4. [Training and Results](#training)\n",
        "5. [Visualizations](#visualizations)\n",
        "6. [Advanced Topics](#advanced)\n",
        "7. [Summary and Next Steps](#summary)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to PPO {#introduction}\n",
        "\n",
        "### What is PPO?\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a policy gradient method that addresses some key limitations of traditional policy gradient algorithms:\n",
        "\n",
        "- **Stability**: Prevents large policy updates that could destabilize training\n",
        "- **Sample Efficiency**: Can use the same data multiple times for better sample efficiency\n",
        "- **Simplicity**: Relatively simple to implement and tune\n",
        "- **Performance**: Achieves state-of-the-art results across many environments\n",
        "\n",
        "### Why PPO?\n",
        "\n",
        "Traditional policy gradient methods like REINFORCE and Actor-Critic can suffer from:\n",
        "\n",
        "1. **High variance** in gradient estimates\n",
        "2. **Sample inefficiency** - each sample is used only once\n",
        "3. **Instability** - large policy updates can lead to performance collapse\n",
        "4. **Hyperparameter sensitivity** - difficult to tune learning rates\n",
        "\n",
        "PPO addresses these issues through its innovative clipped surrogate objective.\n",
        "\n",
        "### Key Innovations\n",
        "\n",
        "1. **Clipped Surrogate Objective**: Prevents destructive policy updates\n",
        "2. **Multiple Epochs**: Reuses data for better sample efficiency\n",
        "3. **Value Function Approximation**: Reduces variance in advantage estimates\n",
        "4. **Entropy Regularization**: Encourages exploration\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mathematical Foundations {#mathematical}\n",
        "\n",
        "### The Clipped Surrogate Objective\n",
        "\n",
        "PPO's core innovation is the clipped surrogate objective:\n",
        "\n",
        "$$L^{CLIP}(\\\\theta) = \\\\mathbb{E}_t \\\\left[ \\\\min(r_t(\\\\theta) A_t, \\\\text{clip}(r_t(\\\\theta), 1-\\\\epsilon, 1+\\\\epsilon) A_t) \\\\right]$$\n",
        "\n",
        "Where:\n",
        "- $r_t(\\\\theta) = \\\\frac{\\\\pi_\\\\theta(a_t|s_t)}{\\\\pi_{\\\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
        "- $A_t$ is the advantage function\n",
        "- $\\\\epsilon$ is the clipping parameter (typically 0.1-0.3)\n",
        "- $\\\\text{clip}(x, a, b) = \\\\max(\\\\min(x, b), a)$ clips the value between $a$ and $b$\n",
        "\n",
        "### Understanding the Clipping\n",
        "\n",
        "The clipping mechanism works as follows:\n",
        "\n",
        "1. **When advantage is positive** ($A_t > 0$):\n",
        "   - We want to increase the probability of this action\n",
        "   - But we limit the increase to prevent over-optimization\n",
        "   - If $r_t(\\\\theta) > 1 + \\\\epsilon$, we clip it to $1 + \\\\epsilon$\n",
        "\n",
        "2. **When advantage is negative** ($A_t < 0$):\n",
        "   - We want to decrease the probability of this action\n",
        "   - But we limit the decrease to prevent over-optimization\n",
        "   - If $r_t(\\\\theta) < 1 - \\\\epsilon$, we clip it to $1 - \\\\epsilon$\n",
        "\n",
        "### Complete PPO Objective\n",
        "\n",
        "The total PPO loss combines multiple components:\n",
        "\n",
        "$$L^{PPO}(\\\\theta) = L^{CLIP}(\\\\theta) - c_1 L^{VF}(\\\\theta) + c_2 S[\\\\pi_\\\\theta](s_t)$$\n",
        "\n",
        "Where:\n",
        "- $L^{CLIP}(\\\\theta)$: Clipped surrogate objective\n",
        "- $L^{VF}(\\\\theta)$: Value function loss (MSE)\n",
        "- $S[\\\\pi_\\\\theta](s_t)$: Entropy bonus for exploration\n",
        "- $c_1, c_2$: Coefficients for value function and entropy terms\n",
        "\n",
        "### Advantage Estimation\n",
        "\n",
        "PPO typically uses Generalized Advantage Estimation (GAE):\n",
        "\n",
        "$$A_t^{GAE(\\\\gamma,\\\\lambda)} = \\\\sum_{l=0}^{\\\\infty} (\\\\gamma\\\\lambda)^l \\\\delta_{t+l}^V$$\n",
        "\n",
        "Where $\\\\delta_t^V = r_t + \\\\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implementation {#implementation}\n",
        "\n",
        "Let's implement PPO step by step. First, we'll set up the environment and imports:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import random\n",
        "from IPython.display import Image, display, HTML\n",
        "import matplotlib.animation as animation\n",
        "from PIL import Image as PILImage\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Imports and setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Setup\n",
        "\n",
        "We'll use the CartPole environment for this tutorial:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(f\"Environment: {env.spec.id}\")\n",
        "print(f\"State size: {state_size}\")\n",
        "print(f\"Action size: {action_size}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Network Architectures\n",
        "\n",
        "PPO uses two networks: an Actor (policy) and a Critic (value function):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"Neural network for policy approximation (Actor)\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, hidden_size=128):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action_probs = F.softmax(self.fc3(x), dim=-1)\n",
        "        return action_probs\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"Neural network for value function approximation (Critic)\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, hidden_size=128):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "        return value\n",
        "\n",
        "print(\"Neural network architectures defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO Agent Implementation\n",
        "\n",
        "Now let's implement the complete PPO agent:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOAgent:\n",
        "    \"\"\"Proximal Policy Optimization implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, state_size, action_size, lr_actor=3e-4, lr_critic=1e-3, gamma=0.99, \n",
        "                 clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01, epochs=4):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.value_coef = value_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.epochs = epochs\n",
        "        \n",
        "        # Networks\n",
        "        self.actor = PolicyNetwork(state_size, action_size).to(device)\n",
        "        self.critic = ValueNetwork(state_size).to(device)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
        "        \n",
        "        # Experience storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        \n",
        "    def act(self, state):\n",
        "        \"\"\"Select action using current policy\"\"\"\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            action_probs = self.actor(state)\n",
        "            value = self.critic(state)\n",
        "            \n",
        "        action_dist = torch.distributions.Categorical(action_probs)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        \n",
        "        return action.item(), log_prob.item(), value.item()\n",
        "    \n",
        "    def store_reward(self, reward, done):\n",
        "        \"\"\"Store reward and done flag for the last action\"\"\"\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "    \n",
        "    def calculate_advantages(self):\n",
        "        \"\"\"Calculate advantages using GAE\"\"\"\n",
        "        advantages = []\n",
        "        returns = []\n",
        "        \n",
        "        # Calculate returns\n",
        "        running_return = 0\n",
        "        for reward, done in zip(reversed(self.rewards), reversed(self.dones)):\n",
        "            running_return = reward + self.gamma * running_return * (1 - done)\n",
        "            returns.insert(0, running_return)\n",
        "        \n",
        "        # Calculate advantages\n",
        "        for i, (return_val, value) in enumerate(zip(returns, self.values)):\n",
        "            advantage = return_val - value\n",
        "            advantages.append(advantage)\n",
        "        \n",
        "        return advantages, returns\n",
        "    \n",
        "    def update(self):\n",
        "        \"\"\"Update networks using PPO\"\"\"\n",
        "        if len(self.states) == 0:\n",
        "            return\n",
        "        \n",
        "        # Calculate advantages and returns\n",
        "        advantages, returns = self.calculate_advantages()\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.stack(self.states).to(device)\n",
        "        actions = torch.LongTensor(self.actions).to(device)\n",
        "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
        "        advantages = torch.FloatTensor(advantages).to(device)\n",
        "        returns = torch.FloatTensor(returns).to(device)\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "        \n",
        "        # Store old policy for ratio calculation\n",
        "        old_policy = PolicyNetwork(self.state_size, self.action_size).to(device)\n",
        "        old_policy.load_state_dict(self.actor.state_dict())\n",
        "        \n",
        "        # PPO updates\n",
        "        for _ in range(self.epochs):\n",
        "            # Get current policy outputs\n",
        "            action_probs = self.actor(states)\n",
        "            values = self.critic(states).squeeze()\n",
        "            \n",
        "            # Calculate policy loss\n",
        "            action_dist = torch.distributions.Categorical(action_probs)\n",
        "            log_probs = action_dist.log_prob(actions)\n",
        "            \n",
        "            # Calculate probability ratio\n",
        "            ratio = torch.exp(log_probs - old_log_probs)\n",
        "            \n",
        "            # Calculate clipped surrogate loss\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            \n",
        "            # Calculate value loss\n",
        "            value_loss = F.mse_loss(values, returns)\n",
        "            \n",
        "            # Calculate entropy loss\n",
        "            entropy_loss = -action_dist.entropy().mean()\n",
        "            \n",
        "            # Total loss\n",
        "            total_loss = actor_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
        "            \n",
        "            # Update networks\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "            self.critic_optimizer.step()\n",
        "        \n",
        "        # Clear storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        \n",
        "        return actor_loss.item(), value_loss.item()\n",
        "\n",
        "print(\"PPO agent implementation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Function\n",
        "\n",
        "Now let's implement the training loop:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ppo(env, agent, episodes=1000, max_steps=500, update_frequency=20):\n",
        "    \"\"\"Train PPO agent\"\"\"\n",
        "    scores = []\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "    \n",
        "    for episode in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action, log_prob, value = agent.act(state)\n",
        "            \n",
        "            # Take action\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store experience\n",
        "            agent.states.append(torch.FloatTensor(state).to(device))\n",
        "            agent.actions.append(action)\n",
        "            agent.log_probs.append(log_prob)\n",
        "            agent.values.append(value)\n",
        "            agent.store_reward(reward, done)\n",
        "            \n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            \n",
        "            # Update networks\n",
        "            if (step + 1) % update_frequency == 0 or done:\n",
        "                actor_loss, critic_loss = agent.update()\n",
        "                if actor_loss is not None:\n",
        "                    actor_losses.append(actor_loss)\n",
        "                    critic_losses.append(critic_loss)\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        scores.append(total_reward)\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            avg_score = np.mean(scores[-100:])\n",
        "            print(f\"Episode {episode}, Average Score: {avg_score:.2f}\")\n",
        "    \n",
        "    return scores, actor_losses, critic_losses\n",
        "\n",
        "print(\"PPO training function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training and Results {#training}\n",
        "\n",
        "Let's train our PPO agent and analyze the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PPO agent\n",
        "ppo_agent = PPOAgent(state_size, action_size, lr_actor=3e-4, lr_critic=1e-3, \n",
        "                     gamma=0.99, clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01, epochs=4)\n",
        "print(\"PPO agent created!\")\n",
        "\n",
        "# Train PPO agent\n",
        "print(\"Training PPO agent...\")\n",
        "ppo_scores, ppo_actor_losses, ppo_critic_losses = train_ppo(env, ppo_agent, episodes=1000, max_steps=500)\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting Results\n",
        "\n",
        "Let's visualize the training progress:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Episode scores\n",
        "axes[0].plot(ppo_scores, alpha=0.6, color='blue')\n",
        "axes[0].plot([np.mean(ppo_scores[max(0, i-100):i+1]) for i in range(len(ppo_scores))], color='red', linewidth=2)\n",
        "axes[0].set_title('PPO Training Scores')\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].grid(True)\n",
        "axes[0].legend(['Episode Score', '100-Episode Average'])\n",
        "\n",
        "# Actor losses\n",
        "if ppo_actor_losses:\n",
        "    axes[1].plot(ppo_actor_losses, color='green')\n",
        "    axes[1].set_title('PPO Actor Loss')\n",
        "    axes[1].set_xlabel('Update Step')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].grid(True)\n",
        "\n",
        "# Critic losses\n",
        "if ppo_critic_losses:\n",
        "    axes[2].plot(ppo_critic_losses, color='orange')\n",
        "    axes[2].set_title('PPO Critic Loss')\n",
        "    axes[2].set_xlabel('Update Step')\n",
        "    axes[2].set_ylabel('Loss')\n",
        "    axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final statistics\n",
        "print(f\"\\nFinal 100-episode average: {np.mean(ppo_scores[-100:]):.2f}\")\n",
        "print(f\"Best 100-episode average: {np.max([np.mean(ppo_scores[max(0, i-100):i+1]) for i in range(100, len(ppo_scores))]):.2f}\")\n",
        "print(f\"Maximum single episode score: {np.max(ppo_scores):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations {#visualizations}\n",
        "\n",
        "Let's create animated visualizations to see our PPO agent in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_cartpole_gif(states, actions, rewards, title=\"CartPole PPO Agent\", max_frames=100, gif_filename=\"cartpole_ppo.gif\"):\n",
        "    \"\"\"Create animated GIF of CartPole episode\"\"\"\n",
        "    # Create rendering environment\n",
        "    render_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "    \n",
        "    # Limit frames for GIF\n",
        "    n_frames = min(len(states), max_frames)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.set_xlim(-2.5, 2.5)\n",
        "    ax.set_ylim(-0.5, 2.5)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Initialize plot elements\n",
        "    cart_width = 0.3\n",
        "    cart_height = 0.2\n",
        "    pole_length = 1.0\n",
        "    \n",
        "    cart = plt.Rectangle((0, 0), cart_width, cart_height, \n",
        "                        facecolor='blue', edgecolor='black', linewidth=2)\n",
        "    pole = plt.Line2D([0, 0], [cart_height/2, cart_height/2 + pole_length], \n",
        "                     color='red', linewidth=4)\n",
        "    \n",
        "    ax.add_patch(cart)\n",
        "    ax.add_line(pole)\n",
        "    \n",
        "    # Add ground line\n",
        "    ax.axhline(y=0, color='black', linewidth=2)\n",
        "    \n",
        "    # Text for score\n",
        "    score_text = ax.text(0.02, 0.98, '', transform=ax.transAxes, \n",
        "                        verticalalignment='top', fontsize=12, \n",
        "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    def animate(frame):\n",
        "        if frame < n_frames:\n",
        "            state = states[frame]\n",
        "            action = actions[frame] if frame < len(actions) else 0\n",
        "            \n",
        "            # Extract state components\n",
        "            cart_pos = state[0]\n",
        "            pole_angle = state[2]\n",
        "            \n",
        "            # Update cart position\n",
        "            cart.set_x(cart_pos - cart_width/2)\n",
        "            \n",
        "            # Update pole angle\n",
        "            pole_x = cart_pos\n",
        "            pole_y = cart_height/2\n",
        "            pole_end_x = pole_x + pole_length * np.sin(pole_angle)\n",
        "            pole_end_y = pole_y + pole_length * np.cos(pole_angle)\n",
        "            \n",
        "            pole.set_data([pole_x, pole_end_x], [pole_y, pole_end_y])\n",
        "            \n",
        "            # Update score\n",
        "            total_reward = sum(rewards[:frame+1])\n",
        "            score_text.set_text(f'Step: {frame+1}\\\\nAction: {action}\\\\nReward: {total_reward:.1f}')\n",
        "            \n",
        "            # Color code based on action\n",
        "            if action == 0:\n",
        "                cart.set_facecolor('lightblue')\n",
        "            else:\n",
        "                cart.set_facecolor('lightcoral')\n",
        "        \n",
        "        return cart, pole, score_text\n",
        "    \n",
        "    # Create animation\n",
        "    anim = animation.FuncAnimation(fig, animate, frames=n_frames, \n",
        "                                 interval=100, blit=False, repeat=True)\n",
        "    \n",
        "    # Save as GIF\n",
        "    anim.save(gif_filename, writer='pillow', fps=10)\n",
        "    \n",
        "    plt.close()\n",
        "    return anim, fig, gif_filename\n",
        "\n",
        "def render_cartpole_episode(env, agent, max_steps=500, title=\"CartPole PPO Agent\"):\n",
        "    \"\"\"Render a single CartPole episode\"\"\"\n",
        "    state, _ = env.reset()\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        action, _, _ = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        states.append(next_state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return np.array(states), np.array(actions), np.array(rewards)\n",
        "\n",
        "print(\"Visualization functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create rendering environments\n",
        "render_env_random = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "render_env_trained = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "# Render random agent behavior\n",
        "print(\"=== CREATING GIF: Random Agent (Before Training) ===\")\n",
        "random_states, random_actions, random_rewards = render_cartpole_episode(\n",
        "    render_env_random, agent=None, max_steps=200, title=\"CartPole Random Agent\"\n",
        ")\n",
        "\n",
        "print(f\"Random Agent Episode Length: {len(random_rewards)} steps\")\n",
        "print(f\"Random Agent Total Reward: {sum(random_rewards)}\")\n",
        "\n",
        "# Create animated GIF for random agent\n",
        "random_anim, random_fig, random_gif_filename = create_cartpole_gif(\n",
        "    random_states, random_actions, random_rewards,\n",
        "    title=\"CartPole Random Agent - Before Training\", max_frames=50, gif_filename=\"cartpole_random_agent.gif\"\n",
        ")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Render trained PPO agent behavior\n",
        "print(\"\\\\n=== CREATING GIF: PPO Agent (After Training) ===\")\n",
        "ppo_states, ppo_actions, ppo_rewards = render_cartpole_episode(\n",
        "    render_env_trained, ppo_agent, max_steps=500, title=\"CartPole PPO Agent\"\n",
        ")\n",
        "\n",
        "print(f\"PPO Agent Episode Length: {len(ppo_rewards)} steps\")\n",
        "print(f\"PPO Agent Total Reward: {sum(ppo_rewards)}\")\n",
        "\n",
        "# Create animated GIF for trained agent\n",
        "ppo_anim, ppo_fig, ppo_gif_filename = create_cartpole_gif(\n",
        "    ppo_states, ppo_actions, ppo_rewards,\n",
        "    title=\"CartPole PPO Agent - After Training\", max_frames=100, gif_filename=\"cartpole_ppo_agent.gif\"\n",
        ")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the animated GIFs\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"CARTPOLE ANIMATED GIF VISUALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display Random Agent GIF\n",
        "print(\"\\\\nüé≤ RANDOM AGENT (Before Training):\")\n",
        "print(\"Watch the pole fall due to random actions...\")\n",
        "display(Image(filename=random_gif_filename))\n",
        "\n",
        "# Display PPO Agent GIF\n",
        "print(\"\\\\nüß† PPO AGENT (After Training):\")\n",
        "print(\"Watch the pole stay balanced using learned policy!\")\n",
        "display(Image(filename=ppo_gif_filename))\n",
        "\n",
        "# Performance comparison\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"Random Agent:\")\n",
        "print(f\"  ‚Ä¢ Episode Length: {len(random_rewards)} steps\")\n",
        "print(f\"  ‚Ä¢ Total Reward: {sum(random_rewards)}\")\n",
        "print(f\"  ‚Ä¢ Average Reward: {np.mean(random_rewards):.3f}\")\n",
        "\n",
        "print(f\"\\\\nPPO Agent:\")\n",
        "print(f\"  ‚Ä¢ Episode Length: {len(ppo_rewards)} steps\")\n",
        "print(f\"  ‚Ä¢ Total Reward: {sum(ppo_rewards)}\")\n",
        "print(f\"  ‚Ä¢ Average Reward: {np.mean(ppo_rewards):.3f}\")\n",
        "\n",
        "# Check if solved (CartPole-v1 is solved at 475 average over 100 episodes)\n",
        "solved_threshold = 475\n",
        "final_avg = np.mean(ppo_scores[-100:]) if len(ppo_scores) >= 100 else np.mean(ppo_scores)\n",
        "solved = final_avg >= solved_threshold\n",
        "\n",
        "print(f\"\\\\nüéØ Environment Solved: {'‚úÖ YES' if solved else '‚ùå NO'}\")\n",
        "print(f\"  ‚Ä¢ Final 100-episode average: {final_avg:.2f}\")\n",
        "print(f\"  ‚Ä¢ Required threshold: {solved_threshold}\")\n",
        "\n",
        "if solved:\n",
        "    print(\"\\\\nüéâ Congratulations! The PPO agent successfully solved CartPole!\")\n",
        "else:\n",
        "    print(\"\\\\nüìà The PPO agent shows significant improvement over random actions!\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clean up environments\n",
        "render_env_random.close()\n",
        "render_env_trained.close()\n",
        "env.close()\n",
        "\n",
        "print(\"\\\\nüéâ PPO visualization complete! The animated GIFs demonstrate the learning progress!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced Topics {#advanced}\n",
        "\n",
        "### Hyperparameter Tuning\n",
        "\n",
        "PPO has several important hyperparameters that can significantly affect performance:\n",
        "\n",
        "1. **Clipping Ratio (Œµ)**: Controls how much the policy can change\n",
        "   - Typical range: 0.1 - 0.3\n",
        "   - Smaller values = more conservative updates\n",
        "\n",
        "2. **Learning Rates**: Separate for actor and critic\n",
        "   - Actor: Usually 3e-4 to 1e-3\n",
        "   - Critic: Usually 1e-3 to 3e-3\n",
        "\n",
        "3. **Value Function Coefficient (c1)**: Weight of value function loss\n",
        "   - Typical range: 0.1 - 1.0\n",
        "\n",
        "4. **Entropy Coefficient (c2)**: Encourages exploration\n",
        "   - Typical range: 0.0 - 0.1\n",
        "\n",
        "5. **GAE Lambda (Œª)**: Controls bias-variance tradeoff in advantage estimation\n",
        "   - Typical range: 0.9 - 0.99\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "1. **High Variance**: Increase clipping ratio or reduce learning rate\n",
        "2. **Slow Learning**: Increase learning rate or reduce clipping ratio\n",
        "3. **Poor Exploration**: Increase entropy coefficient\n",
        "4. **Unstable Training**: Add gradient clipping or reduce learning rate\n",
        "\n",
        "### Extensions and Variants\n",
        "\n",
        "1. **PPO2**: OpenAI's implementation with additional optimizations\n",
        "2. **Distributed PPO**: Multiple workers collecting experience in parallel\n",
        "3. **PPO with LSTM**: For partially observable environments\n",
        "4. **PPO with Curiosity**: Adding intrinsic motivation for exploration\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Next Steps {#summary}\n",
        "\n",
        "### What We've Learned\n",
        "\n",
        "1. **PPO Fundamentals**: Understanding the clipped surrogate objective\n",
        "2. **Implementation**: Complete PPO agent with actor-critic architecture\n",
        "3. **Training**: Effective training loop with experience collection and updates\n",
        "4. **Visualization**: Animated demonstrations of agent behavior\n",
        "5. **Analysis**: Performance metrics and comparison with random baseline\n",
        "\n",
        "### Key Advantages of PPO\n",
        "\n",
        "- **Stability**: Clipped objective prevents destructive updates\n",
        "- **Sample Efficiency**: Multiple epochs of updates on same data\n",
        "- **Simplicity**: Relatively easy to implement and tune\n",
        "- **Performance**: State-of-the-art results on many benchmarks\n",
        "\n",
        "### When to Use PPO\n",
        "\n",
        "- **Continuous control tasks** (robotics, autonomous driving)\n",
        "- **Discrete action spaces** (games, decision making)\n",
        "- **High-dimensional state spaces** (computer vision, NLP)\n",
        "- **When sample efficiency matters** (expensive data collection)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Experiment with hyperparameters** to improve performance\n",
        "2. **Try different environments** (MountainCar, LunarLander, etc.)\n",
        "3. **Implement distributed PPO** for faster training\n",
        "4. **Explore advanced variants** (PPO2, PPO with LSTM)\n",
        "5. **Apply to real-world problems** in your domain\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "- [PPO Paper](https://arxiv.org/abs/1707.06347) - Original PPO paper\n",
        "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ppo.html) - Excellent PPO tutorial\n",
        "- [Stable Baselines3](https://stable-baselines3.readthedocs.io/) - Production-ready PPO implementation\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've successfully implemented and trained a PPO agent. This is a powerful algorithm that forms the foundation for many modern reinforcement learning applications.\n",
        "\n",
        "Happy learning! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
